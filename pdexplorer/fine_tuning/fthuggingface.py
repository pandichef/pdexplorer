from .._dataset import current
from .._print import _print
from .._commandarg import parse
from ._sentiment_analysis import sentiment_analysis
from ._fill_mask import fill_mask

task_map = {  # generated by ChatGPT #
    "audio-classification": "AutoModelForAudioClassification",
    "automatic-speech-recognition": "AutoModelForAutomaticSpeechRecognition",
    "conversational": "AutoModelForSeq2SeqLM",
    "depth-estimation": "AutoModelForImageSegmentation",
    "document-question-answering": "AutoModelForQuestionAnswering",
    "feature-extraction": "AutoModel",
    "fill-mask": "AutoModelForMaskedLM",
    "image-classification": "AutoModelForImageClassification",
    "image-segmentation": "AutoModelForImageSegmentation",
    "image-to-image": "AutoModelForImageSegmentation",
    "image-to-text": "AutoModelForImageToText",
    "mask-generation": "AutoModelForImageSegmentation",
    "ner": "AutoModelForTokenClassification",  # https://huggingface.co/docs/transformers/tasks/token_classification #
    "object-detection": "AutoModelForObjectDetection",
    "question-answering": "AutoModelForQuestionAnswering",
    "sentiment-analysis": "AutoModelForSequenceClassification",
    "summarization": "AutoModelForSeq2SeqLM",
    "table-question-answering": "AutoModelForQuestionAnswering",
    "text-classification": "AutoModelForSequenceClassification",  # https://huggingface.co/docs/transformers/tasks/token_classification #
    "text-generation": "AutoModelForCausalLM",
    "text-to-audio": "AutoModelForTextToSpeech",
    "text-to-speech": "AutoModelForTextToSpeech",
    "text2text-generation": "AutoModelForSeq2SeqLM",
    "token-classification": "AutoModelForTokenClassification",
    "translation": "AutoModelForSeq2SeqLM",
    "video-classification": "AutoModelForVideoClassification",
    "visual-question-answering": "AutoModelForImageTextIntegration",
    "vqa": "AutoModelForImageTextIntegration",
    "zero-shot-audio-classification": "AutoModelForZeroShotClassification",
    "zero-shot-classification": "AutoModelForZeroShotClassification",
    "zero-shot-image-classification": "AutoModelForZeroShotClassification",
    "zero-shot-object-detection": "AutoModelForZeroShotObjectDetection",
    "translation_XX_to_YY": "AutoModelForSeq2SeqLM",
}

# num_columns = {
#     "text-classification": 2,  # text and label
#     "sentiment-analysis": 2,  # text and label
#     "fill-mask": 1,  # text only
# }


# def get_tokenized_data(
#     huggingface_dataset, tokenizer, yvar, xvar=None, task="text-classification"
# ):
#     if task in ["text-classification", "sentiment-analysis"]:

#         def preprocess_function(examples):
#             return tokenizer(examples[xvar], padding="max_length", truncation=True)

#         tokenized_data = huggingface_dataset.map(preprocess_function, batched=True,)
#     elif task in ["fill-mask"]:

#         def preprocess_function(examples):
#             return tokenizer([" ".join(x) for x in examples[yvar]])

#         tokenized_data = huggingface_dataset.map(
#             preprocess_function,
#             batched=True,
#             num_proc=4,
#             remove_columns=huggingface_dataset["train"].column_names,
#         )

#         def group_texts(examples):
#             block_size = 128
#             concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
#             total_length = len(concatenated_examples[list(examples.keys())[0]])
#             if total_length >= block_size:
#                 total_length = (total_length // block_size) * block_size
#             result = {
#                 k: [t[i : i + block_size] for i in range(0, total_length, block_size)]
#                 for k, t in concatenated_examples.items()
#             }
#             return result

#         tokenized_data = tokenized_data.map(group_texts, batched=True, num_proc=4)
#     else:
#         tokenized_data = None
#     return tokenized_data


fnc_map = {
    "text-classification": sentiment_analysis,
    "sentiment-analysis": sentiment_analysis,
    "fill-mask": fill_mask,
}

default_models = {
    "sentiment-analysis": "distilbert-base-uncased",
    "text-classification": "distilbert-base-uncased",
    "fill-mask": "distilroberta-base",
}


def fthuggingface(
    commandarg: str, task="sentiment-analysis", model_name=None  # varlist #
):
    if model_name is None:
        model_name = default_models[task]
    task_fnc = fnc_map[task]
    task_fnc(commandarg=commandarg, model_name=model_name)


def askhuggingface(prompt, task="text-classification"):
    import transformers
    from transformers import (
        pipeline,
        AutoTokenizer,
    )

    _AutoModel = getattr(transformers, task_map[task])
    base_model_name = "-".join(current.last_huggingface_ftmodel_dir.split("-")[1:-1])  # type: ignore
    tokenizer = AutoTokenizer.from_pretrained(base_model_name)
    model = _AutoModel.from_pretrained(current.last_huggingface_ftmodel_dir)
    fine_tuned_text_classification_pipeline = pipeline(
        task, model=model, tokenizer=tokenizer,
    )
    print(fine_tuned_text_classification_pipeline(prompt)[0])  # type: ignore


##################################################
# def fthuggingface_old(
#     commandarg: str,
#     output_dir="output_dir",
#     num_examples=100,
#     model_name="distilbert-base-uncased",
#     num_labels=5,
#     run_in_sample=False,
# ):
#     # Deprecated #
#     _ = parse(commandarg, "varlist")
#     assert len(_["varlist"].split()) == 2
#     yvar = _["varlist"].split()[0]
#     xvar = _["varlist"].split()[1]
#     # https://huggingface.co/docs/transformers/training#train-with-pytorch-trainer
#     import datasets
#     import torch
#     import numpy as np
#     import evaluate
#     from datasets import load_dataset
#     from transformers import (
#         pipeline,
#         AutoTokenizer,
#         AutoModelForSequenceClassification,  # text-classification
#         AutoModelForMaskedLM,  # fill-mask
#         TrainingArguments,
#         Trainer,
#     )

#     tokenizer = AutoTokenizer.from_pretrained(model_name)

#     dataset = datasets.DatasetDict()
#     dataset["train"] = current.get_huggingface_dataset(split="train")
#     dataset["test"] = current.get_huggingface_dataset(split="test")

#     # tokenize the fine-tuning dataset #
#     def tokenize_function(examples):
#         return tokenizer(examples[xvar], padding="max_length", truncation=True)

#     tokenized_datasets = dataset.map(tokenize_function, batched=True,)

#     # sample from fine-tuning dataset #
#     small_train_dataset = (
#         tokenized_datasets["train"].shuffle(seed=42).select(range(num_examples))
#     )
#     small_eval_dataset = (
#         tokenized_datasets["test"].shuffle(seed=42).select(range(num_examples))
#     )

#     # load model for specific application #
#     # e.g., distilbert-base-uncased loaded for text classification with 5 stars #
#     model = AutoModelForSequenceClassification.from_pretrained(
#         model_name, num_labels=num_labels
#     )

#     if run_in_sample:
#         text_classification_pipeline = pipeline(
#             "text-classification", model=model, tokenizer=tokenizer
#         )

#         def _text_classification_pipeline(x):
#             try:
#                 return int(
#                     text_classification_pipeline(x)[0][yvar].replace("LABEL_", "")
#                 )
#             except RuntimeError:
#                 return -1

#         current._df["_pretrained"] = current._df[xvar].apply(
#             _text_classification_pipeline
#         )

#     # train the model
#     def compute_metrics(eval_pred):
#         logits, labels = eval_pred
#         predictions = np.argmax(logits, axis=-1)
#         return evaluate.load("accuracy").compute(
#             predictions=predictions, references=labels
#         )

#     trainer = Trainer(
#         model=model,
#         args=TrainingArguments(
#             output_dir=output_dir,
#             evaluation_strategy="epoch",
#             no_cuda=not torch.cuda.is_available(),
#             # no_cuda=True,
#             report_to="none"
#             # per_device_train_batch_size=8,
#             # per_gpu_eval_batch_size=8,
#         ),
#         train_dataset=small_train_dataset,
#         eval_dataset=small_eval_dataset,
#         compute_metrics=compute_metrics,
#     )
#     trainer.train()
#     trainer.save_model()  # saved to output_dir
#     _print(f"Saved model to ./{output_dir}")

#     if run_in_sample:
#         fine_tuned_text_classification_pipeline = pipeline(
#             "text-classification",
#             model=AutoModelForSequenceClassification.from_pretrained(output_dir),
#             tokenizer=tokenizer,
#         )

#         def _fine_tuned_text_classification_pipeline(x):
#             try:
#                 return int(
#                     fine_tuned_text_classification_pipeline(x)[0][yvar].replace(
#                         "LABEL_", ""
#                     )
#                 )
#             except RuntimeError:
#                 return -1

#         current._df["_finetuned"] = current._df[xvar].apply(
#             _fine_tuned_text_classification_pipeline
#         )
#         _print(current.df)
#     torch.cuda.empty_cache()
