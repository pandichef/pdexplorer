from .._dataset import current
from .._print import _print
from .._commandarg import parse
from ._sentiment_analysis import sentiment_analysis
from ._fill_mask import fill_mask
from ._text_generation import text_generation
from ._audio_classification import audio_classification
from ._translation import translation

task_map = {  # generated by ChatGPT #
    "sentiment-analysis": {
        "auto_model_class": "AutoModelForSequenceClassification",
        "fine_tuner": sentiment_analysis,
        "default_model": "distilbert-base-uncased",
    },
    "text-classification": {
        "auto_model_class": "AutoModelForSequenceClassification",
        "fine_tuner": sentiment_analysis,
        "default_model": "distilbert-base-uncased",
    },
    "fill-mask": {
        "auto_model_class": "AutoModelForMaskedLM",
        "fine_tuner": fill_mask,
        "default_model": "distilroberta-base",
    },
    "text-generation": {
        "auto_model_class": "AutoModelForCausalLM",
        "fine_tuner": text_generation,
        "default_model": "distilgpt2",
    },
    "audio-classification": {
        "auto_model_class": "AutoModelForAudioClassification",
        "fine_tuner": audio_classification,
        "default_model": "facebook/wav2vec2-base",
        # "default_model": "openai/whisper-tiny",
    },
    "translation": {
        "auto_model_class": "AutoModelForSeq2SeqLM",
        "fine_tuner": translation,
        "default_model": "t5-small",
    },
    "automatic-speech-recognition": {
        "auto_model_class": "AutoModelForAutomaticSpeechRecognition"
    },
    "conversational": {"auto_model_class": "AutoModelForSeq2SeqLM"},
    "depth-estimation": {"auto_model_class": "AutoModelForImageSegmentation"},
    "document-question-answering": {
        "auto_model_class": "AutoModelForQuestionAnswering"
    },
    "feature-extraction": {"auto_model_class": "AutoModel"},
    "image-classification": {"auto_model_class": "AutoModelForImageClassification"},
    "image-segmentation": {"auto_model_class": "AutoModelForImageSegmentation"},
    "image-to-image": {"auto_model_class": "AutoModelForImageSegmentation"},
    "image-to-text": {"auto_model_class": "AutoModelForImageToText"},
    "mask-generation": {"auto_model_class": "AutoModelForImageSegmentation"},
    "ner": {"auto_model_class": "AutoModelForTokenClassification"},
    "object-detection": {"auto_model_class": "AutoModelForObjectDetection"},
    "question-answering": {"auto_model_class": "AutoModelForQuestionAnswering"},
    "summarization": {"auto_model_class": "AutoModelForSeq2SeqLM"},
    "table-question-answering": {"auto_model_class": "AutoModelForQuestionAnswering"},
    "text-to-audio": {"auto_model_class": "AutoModelForTextToSpeech"},
    "text-to-speech": {"auto_model_class": "AutoModelForTextToSpeech"},
    "text2text-generation": {"auto_model_class": "AutoModelForSeq2SeqLM"},
    "token-classification": {"auto_model_class": "AutoModelForTokenClassification"},
    "video-classification": {"auto_model_class": "AutoModelForVideoClassification"},
    "visual-question-answering": {
        "auto_model_class": "AutoModelForImageTextIntegration"
    },
    "vqa": {"auto_model_class": "AutoModelForImageTextIntegration"},
    "zero-shot-audio-classification": {
        "auto_model_class": "AutoModelForZeroShotClassification"
    },
    "zero-shot-classification": {
        "auto_model_class": "AutoModelForZeroShotClassification"
    },
    "zero-shot-image-classification": {
        "auto_model_class": "AutoModelForZeroShotClassification"
    },
    "zero-shot-object-detection": {
        "auto_model_class": "AutoModelForZeroShotObjectDetection"
    },
    "translation_XX_to_YY": {"auto_model_class": "AutoModelForSeq2SeqLM"},
}


def fthuggingface(
    commandarg: str, task="sentiment-analysis", model_name=None  # varlist #
):
    if model_name is None:
        model_name = task_map[task]["default_model"]
    fine_tuner_function = task_map[task]["fine_tuner"]
    fine_tuner_function(commandarg=commandarg, model_name=model_name)


def askhuggingface(
    prompt,
    task="text-classification",
    source_lang="en",  # for translation only #
    target_lang="fr",  # for translation only #
):
    import transformers
    from transformers import (
        pipeline,
        AutoTokenizer,
        AutoFeatureExtractor,  # for audio-classification
    )

    _AutoModel = getattr(transformers, task_map[task]["auto_model_class"])
    base_model_name = "-".join(current.last_huggingface_ftmodel_dir.split("-")[1:-1]).replace("_", "/")  # type: ignore

    model = _AutoModel.from_pretrained(current.last_huggingface_ftmodel_dir)
    pipeline_kwargs = {
        "task": task,
        "model": model,
    }

    if task == "audio-classification":
        feature_extractor = AutoFeatureExtractor.from_pretrained(base_model_name)
        pipeline_kwargs.update({"feature_extractor": feature_extractor})
    else:
        tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        pipeline_kwargs.update({"tokenizer": tokenizer})

    fine_tuned_text_classification_pipeline = pipeline(**pipeline_kwargs)

    if task == "translation":
        from ._translation import iso_639_1

        prefix = f"translate {iso_639_1[source_lang]} to {iso_639_1[target_lang]}: "
        prompt = prefix + prompt

    result = fine_tuned_text_classification_pipeline(prompt)
    # print()  # type: ignore
    return result


##################################################
# def fthuggingface_old(
#     commandarg: str,
#     output_dir="output_dir",
#     num_examples=100,
#     model_name="distilbert-base-uncased",
#     num_labels=5,
#     run_in_sample=False,
# ):
#     # Deprecated #
#     _ = parse(commandarg, "varlist")
#     assert len(_["varlist"].split()) == 2
#     yvar = _["varlist"].split()[0]
#     xvar = _["varlist"].split()[1]
#     # https://huggingface.co/docs/transformers/training#train-with-pytorch-trainer
#     import datasets
#     import torch
#     import numpy as np
#     import evaluate
#     from datasets import load_dataset
#     from transformers import (
#         pipeline,
#         AutoTokenizer,
#         AutoModelForSequenceClassification,  # text-classification
#         AutoModelForMaskedLM,  # fill-mask
#         TrainingArguments,
#         Trainer,
#     )

#     tokenizer = AutoTokenizer.from_pretrained(model_name)

#     dataset = datasets.DatasetDict()
#     dataset["train"] = current.get_huggingface_dataset(split="train")
#     dataset["test"] = current.get_huggingface_dataset(split="test")

#     # tokenize the fine-tuning dataset #
#     def tokenize_function(examples):
#         return tokenizer(examples[xvar], padding="max_length", truncation=True)

#     tokenized_datasets = dataset.map(tokenize_function, batched=True,)

#     # sample from fine-tuning dataset #
#     small_train_dataset = (
#         tokenized_datasets["train"].shuffle(seed=42).select(range(num_examples))
#     )
#     small_eval_dataset = (
#         tokenized_datasets["test"].shuffle(seed=42).select(range(num_examples))
#     )

#     # load model for specific application #
#     # e.g., distilbert-base-uncased loaded for text classification with 5 stars #
#     model = AutoModelForSequenceClassification.from_pretrained(
#         model_name, num_labels=num_labels
#     )

#     if run_in_sample:
#         text_classification_pipeline = pipeline(
#             "text-classification", model=model, tokenizer=tokenizer
#         )

#         def _text_classification_pipeline(x):
#             try:
#                 return int(
#                     text_classification_pipeline(x)[0][yvar].replace("LABEL_", "")
#                 )
#             except RuntimeError:
#                 return -1

#         current._df["_pretrained"] = current._df[xvar].apply(
#             _text_classification_pipeline
#         )

#     # train the model
#     def compute_metrics(eval_pred):
#         logits, labels = eval_pred
#         predictions = np.argmax(logits, axis=-1)
#         return evaluate.load("accuracy").compute(
#             predictions=predictions, references=labels
#         )

#     trainer = Trainer(
#         model=model,
#         args=TrainingArguments(
#             output_dir=output_dir,
#             evaluation_strategy="epoch",
#             no_cuda=not torch.cuda.is_available(),
#             # no_cuda=True,
#             report_to="none"
#             # per_device_train_batch_size=8,
#             # per_gpu_eval_batch_size=8,
#         ),
#         train_dataset=small_train_dataset,
#         eval_dataset=small_eval_dataset,
#         compute_metrics=compute_metrics,
#     )
#     trainer.train()
#     trainer.save_model()  # saved to output_dir
#     _print(f"Saved model to ./{output_dir}")

#     if run_in_sample:
#         fine_tuned_text_classification_pipeline = pipeline(
#             "text-classification",
#             model=AutoModelForSequenceClassification.from_pretrained(output_dir),
#             tokenizer=tokenizer,
#         )

#         def _fine_tuned_text_classification_pipeline(x):
#             try:
#                 return int(
#                     fine_tuned_text_classification_pipeline(x)[0][yvar].replace(
#                         "LABEL_", ""
#                     )
#                 )
#             except RuntimeError:
#                 return -1

#         current._df["_finetuned"] = current._df[xvar].apply(
#             _fine_tuned_text_classification_pipeline
#         )
#         _print(current.df)
#     torch.cuda.empty_cache()
